{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Faça um crawler que navegue pelas páginas de países e baixe os htmls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Example web scraping website\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLog In Sign Up Log In\\n\\nHomeSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Example web scraping website\\n                    \\n\\n\\n\\n\\n\\n\\n\\n Afghanistan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda\\n\\n\\n\\n    < Previous\\n\\n|\\n\\n    Next >', 'Example web scraping website', 'Example web scraping website', '', '', '', '', '', '', '<!--\\n    // These variables are used by the web2py_ajax_init function in web2py_ajax.js (which is loaded below).\\n    var w2p_ajax_confirm_message = \"Are you sure you want to delete this object?\";\\n    var w2p_ajax_disable_with_message = \"Working...\";\\n    var w2p_ajax_date_format = \"%Y-%m-%d\";\\n    var w2p_ajax_datetime_format = \"%Y-%m-%d %H:%M:%S\";\\n    var ajax_error_500 = \\'An error occured, please <a href=\"/places/default/index/0\">reload</a> the page\\'\\n    //-->', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Log In Sign Up Log In\\n\\nHomeSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    Example web scraping website\\n                    \\n\\n\\n\\n\\n\\n\\n\\n Afghanistan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda\\n\\n\\n\\n    < Previous\\n\\n|\\n\\n    Next >', 'Log In Sign Up Log In\\n\\nHomeSearch', '', 'Log In Sign Up Log In\\n\\nHomeSearch', 'Log In Sign Up Log In\\n\\nHomeSearch', '', '', '', '', 'Log In Sign Up Log In', 'Log In Sign Up Log In', 'Log In', 'Sign Up Log In', 'Sign Up', 'Sign Up', '', '', 'Log In', 'Log In', '', 'HomeSearch', 'HomeSearch', 'Home', 'Home', 'Search', 'Search', 'Example web scraping website\\n                    \\n\\n\\n\\n\\n\\n\\n\\n Afghanistan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda\\n\\n\\n\\n    < Previous\\n\\n|\\n\\n    Next >', 'Example web scraping website', 'Example web scraping website', 'Example web scraping website', 'Example web scraping website', '', 'Afghanistan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda\\n\\n\\n\\n    < Previous\\n\\n|\\n\\n    Next >', 'Afghanistan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda\\n\\n\\n\\n    < Previous\\n\\n|\\n\\n    Next >', 'Afghanistan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda', 'Afghanistan Aland Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda', 'Afghanistan Aland Islands', 'Afghanistan', 'Afghanistan', 'Afghanistan', '', 'Aland Islands', 'Aland Islands', 'Aland Islands', '', 'Albania Algeria', 'Albania', 'Albania', 'Albania', '', 'Algeria', 'Algeria', 'Algeria', '', 'American Samoa Andorra', 'American Samoa', 'American Samoa', 'American Samoa', '', 'Andorra', 'Andorra', 'Andorra', '', 'Angola Anguilla', 'Angola', 'Angola', 'Angola', '', 'Anguilla', 'Anguilla', 'Anguilla', '', 'Antarctica Antigua and Barbuda', 'Antarctica', 'Antarctica', 'Antarctica', '', 'Antigua and Barbuda', 'Antigua and Barbuda', 'Antigua and Barbuda', '', '< Previous\\n\\n|\\n\\n    Next >', 'Next >', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "r = requests.get('http://127.0.0.1:8000/places/default/index/0')\n",
    "soup = BeautifulSoup(r.text)\n",
    "\n",
    "print([tag.text.strip() for tag in soup.find_all()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLog In Sign Up Log In\\n\\nHomeSearch\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.body.div.text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Faça scraping dos htmls baixados e salve os dados retirados em um arquivo csv. Salvar uma coluna extra no csv contendo um timestamp do momento no qual os dados foram obtidos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Faça um crawler que monitore as páginas de países e procure por atualizações. Caso algum registro tenha sido atualizado esse deve ser atualizado no arquivo CSV, caso contrário manter a versão anterior."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bca09b3ec0e3b8add89f2defdb11a3c6b8091e5e2b4b672bc9d80f59693e8458"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
