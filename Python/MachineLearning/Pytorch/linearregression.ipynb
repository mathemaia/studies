{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Perceptron(object):\n",
    "    def __init__(self, n_inputs):\n",
    "        self.weights = np.zeros(n_inputs + 1)\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        if summation > 0:\n",
    "            activation = summation\n",
    "        else:\n",
    "            activation = 0\n",
    "        return activation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitetura da Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Permite utilizar o módulo com todas as utilidades da rede\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# Arquitetura\n",
    "class NeuralNetwork(nn.Module):\n",
    "    # Inicialização\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Linear(1, 1)\n",
    "\n",
    "    # Computação\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação da infraestrutura de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe responsável por criar o dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Separa aleatoriamente amostras em um intervalo\n",
    "import torch.distributions.uniform as urand\n",
    "\n",
    "\n",
    "# Dataset\n",
    "class AlgebricDataset(Dataset):\n",
    "    def __init__(self, f, interval, n_samples):\n",
    "        X = urand.Uniform(interval[0], interval[1]).sample([n_samples]) # Amostra pares ordenados de forma uniforme \n",
    "        self.data = [(x, f(x)) for x in X]\n",
    "    \n",
    "    # Diz quantos dados tem no dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # Busca dados pelo índice\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "# Parâmetros para utilizar na rede\n",
    "line = lambda x: 2 * x + 3\n",
    "interval = (-10, 10)\n",
    "train_samples = 1000\n",
    "test_samples = 100\n",
    "\n",
    "\n",
    "# Separação do dataset de treino e de teste\n",
    "train_dataset = AlgebricDataset(line, interval, train_samples)\n",
    "test_dataset = AlgebricDataset(line, interval, test_samples)\n",
    "\n",
    "\n",
    "# Bloco responsável por carregar os dados\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=train_samples, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=test_samples, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparâmetros de Otimização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica se existe uma GPU disponível, caso contrário utiliza a CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Device used: {device}')\n",
    "\n",
    "\n",
    "# Instancia o modelo\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "\n",
    "# Função de perda\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Gradiente descendente estocástico\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de treinamento e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de treinamento\n",
    "def train(model, dataloader, loss, optimizer):\n",
    "    # Garante que o modelo vai ser treinado\n",
    "    model.train()\n",
    "\n",
    "    # Erro acumulado\n",
    "    cumulative_loss = 0.0\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        # [1, 2, 3, 4] --> [[1.0], [2.0], [3.0], [4.0]]\n",
    "        X = X.unsqueeze(1).float().to(device) \n",
    "        y = y.unsqueeze(1).float().to(device)\n",
    "\n",
    "        # Predição\n",
    "        pred = model(X)\n",
    "\n",
    "        # Erro\n",
    "        loss = loss_func(pred, y)\n",
    "\n",
    "        # Zera os gradientes acumulados\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculo dos gradientes (back propagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Anda na direção do gradiente\n",
    "        optimizer.step()\n",
    "\n",
    "        # Acumula o valor de perda\n",
    "        cumulative_loss += loss.item()\n",
    "    return cumulative_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# Função de teste\n",
    "def test(model, dataloader, loss):\n",
    "    # Garante que o modelo já otimizado vai ser testado\n",
    "    model.eval()\n",
    "\n",
    "    # Erro acumulado\n",
    "    cumulative_loss = 0.0\n",
    "\n",
    "    # Garante que o modelo não vai acumular os valores do gradiente\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # [1, 2, 3, 4] --> [[1.0], [2.0], [3.0], [4.0]]\n",
    "            X = X.unsqueeze(1).float().to(device) \n",
    "            y = y.unsqueeze(1).float().to(device)\n",
    "\n",
    "            # Predição\n",
    "            pred = model(X)\n",
    "\n",
    "            # Erro\n",
    "            loss = loss_func(pred, y)\n",
    "\n",
    "            # Acumula o valor de do ambienteperda\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "    return cumulative_loss / len(dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 101\n",
    "for t in range(epochs):\n",
    "    train_loss = train(model, train_dataloader, loss_func, optimizer)\n",
    "    if t % 10 == 0:\n",
    "        print(f'Epoch: {t}\\nTrain loss: {train_loss}\\n')\n",
    "\n",
    "test_loss = test(model, test_dataloader, loss_func)\n",
    "print(f'Test loss: {test_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
