{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Conv2D, Conv2DTranspose, Input, Flatten, BatchNormalization, Lambda, Reshape, Activation\n",
    "from keras.layers import LeakyReLU, Multiply, Add\n",
    "from keras.activations import selu\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/3053_2002.jpg', '../data/2732_2002.jpg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir = '../data/'\n",
    "images = [os.path.join(image_dir, image) for image in os.listdir(image_dir)]\n",
    "\n",
    "images[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 64\n",
    "\n",
    "def preprocess(image):\n",
    "    # read and convert the image into utf8 tensor\n",
    "    image = tf.io.read_file(image)\n",
    "\n",
    "    # decode the image into numpy array tensor\n",
    "    image = tf.io.decode_jpeg(image)\n",
    "\n",
    "    # convert the type\n",
    "    image = tf.cast(image, tf.float32)\n",
    "\n",
    "    # resize the image\n",
    "    image = tf.image.resize(image, (image_size, image_size))\n",
    "\n",
    "    # normalize the image\n",
    "    image = image / 255.0\n",
    "\n",
    "    # rehape the image\n",
    "    image = tf.reshape(image, shape=(image_size, image_size, 3))\n",
    "\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# divide the dataset in batches\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((images))\n",
    "\n",
    "# apply a map function to turn each image in an numpy array tensor\n",
    "training_dataset = training_dataset.map(preprocess)\n",
    "\n",
    "# shuffle the dataset and configure to load each batch at a time\n",
    "training_dataset = training_dataset.shuffle(1000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "# Hiperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(y, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y - y_pred))\n",
    "\n",
    "def kl_loss(mu, log_var):\n",
    "    return -0.5 * tf.reduce_mean(1 + log_var - tf.square(mu) - tf.exp(log_var))\n",
    "\n",
    "def var_loss(y, y_pred, mu, log_var):\n",
    "    return reconstruction_loss(y, y_pred) + (1 / (64 * 64)) * kl_loss(mu, log_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Variational Autoencoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 64, 64, 3)]          0         []                            \n",
      "                                                                                                  \n",
      " encoder (Functional)        [(None, 512),                1380268   ['input_1[0][0]']             \n",
      "                              (None, 512),                8                                       \n",
      "                              (None, 512)]                                                        \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, 64, 64, 3)            1328302   ['encoder[0][2]']             \n",
      "                                                          3                                       \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 512)                  0         ['encoder[0][1]']             \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.math.square (TFOpLambda  (None, 512)                  0         ['encoder[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.subtract (TFOpLamb  (None, 512)                  0         ['tf.__operators__.add[0][0]',\n",
      " da)                                                                 'tf.math.square[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.exp_1 (TFOpLambda)  (None, 512)                  0         ['encoder[0][1]']             \n",
      "                                                                                                  \n",
      " tf.math.subtract_1 (TFOpLa  (None, 512)                  0         ['tf.math.subtract[0][0]',    \n",
      " mbda)                                                               'tf.math.exp_1[0][0]']       \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpL  ()                           0         ['tf.math.subtract_1[0][0]']  \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLa  ()                           0         ['tf.math.reduce_mean[0][0]'] \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " add_loss (AddLoss)          ()                           0         ['tf.math.multiply_1[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 27085711 (103.32 MB)\n",
      "Trainable params: 27078665 (103.30 MB)\n",
      "Non-trainable params: 7046 (27.52 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "latent_dim = 512\n",
    "\n",
    "\n",
    "## ENCODER\n",
    "encoder_input = Input(shape=(64, 64, 3))\n",
    "x = Conv2D(32, kernel_size=5, activation=LeakyReLU(0.02), strides=1, padding='same')(encoder_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, kernel_size=5, activation=LeakyReLU(0.02), strides=2, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, kernel_size=5, activation=LeakyReLU(0.02), strides=2, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(256, kernel_size=5, activation=LeakyReLU(0.02), strides=2, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(512, kernel_size=5, activation=LeakyReLU(0.02), strides=2, padding='same')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=selu)(x)\n",
    "\n",
    "\n",
    "## LATENT\n",
    "encoder_output = BatchNormalization()(x)\n",
    "\n",
    "mu = Dense(latent_dim)(encoder_output)\n",
    "log_var = Dense(latent_dim)(encoder_output)\n",
    "\n",
    "epsilon = K.random_normal(shape=(tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "sigma = tf.exp(0.5 * log_var)\n",
    "\n",
    "z_eps = Multiply()([sigma, epsilon])\n",
    "z = Add()([mu, z_eps])\n",
    "\n",
    "encoder = Model(encoder_input, outputs=[mu, log_var, z], name='encoder')\n",
    "\n",
    "\n",
    "## DECODER\n",
    "decoder = Sequential()\n",
    "decoder.add(Dense(1024, activation=selu, input_shape=(latent_dim,)))\n",
    "decoder.add(BatchNormalization())\n",
    "\n",
    "decoder.add(Dense(8192, activation=selu))\n",
    "decoder.add(Reshape((4, 4, 512)))\n",
    "\n",
    "decoder.add(Conv2DTranspose(256, (5, 5), activation=LeakyReLU(0.02), strides=2, padding='same'))\n",
    "decoder.add(BatchNormalization())\n",
    "\n",
    "decoder.add(Conv2DTranspose(128, (5, 5), activation=LeakyReLU(0.02), strides=2, padding='same'))\n",
    "decoder.add(BatchNormalization())\n",
    "\n",
    "decoder.add(Conv2DTranspose(64, (5, 5), activation=LeakyReLU(0.02), strides=2, padding='same'))\n",
    "decoder.add(BatchNormalization())\n",
    "\n",
    "decoder.add(Conv2DTranspose(32, (5, 5), activation=LeakyReLU(0.02), strides=2, padding='same'))\n",
    "decoder.add(BatchNormalization())\n",
    "\n",
    "decoder.add(Conv2DTranspose(3, (5, 5), activation='sigmoid', strides=1, padding='same'))\n",
    "decoder.add(BatchNormalization())\n",
    "\n",
    "\n",
    "## CONNECT ENCODER AND DECODER\n",
    "mu, log_var, z = encoder(encoder_input)\n",
    "reconstructed = decoder(z)\n",
    "model = Model(encoder_input, reconstructed, name='Variational Autoencoder')\n",
    "loss = kl_loss(mu, log_var)\n",
    "model.add_loss(loss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = (model(preprocess(images[0])) * 255.0)[0].numpy().astype(int)\n",
    "\n",
    "print(img.shape)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "# Saving images during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(model, epoch, step, input_):\n",
    "    prediction = model.predict(input_)\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(14, 14))\n",
    "    idx = 0\n",
    "    for row in range(5):\n",
    "        for column in range(5):\n",
    "            image = prediction[idx] * 255\n",
    "            image = image.astype('int32')\n",
    "            axes[row, column].imshow(image)\n",
    "            axes[row, column].axis('off')\n",
    "            idx += 1\n",
    "    output_path = 'output/'\n",
    "\n",
    "    if not os.path.exists(output_path):\n",
    "        os.mkdir(output_path)\n",
    "    plt.savefig(output_path + 'Epoch_{:04d}_step_{:04d}.jpg'.format(epoch, step))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 414ms/step\n",
      "Epoch:  1\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "Epoch: 1 - Step: 0 - MSE loss: 0.09589151 - KL loss: 0.001586138\n",
      "Epoch: 1 - Step: 1 - MSE loss: 0.097878836 - KL loss: 0.0011765136\n",
      "Epoch: 1 - Step: 2 - MSE loss: 0.09153416 - KL loss: 0.000850165\n",
      "Epoch: 1 - Step: 3 - MSE loss: 0.0963415 - KL loss: 0.0006176908\n",
      "Epoch:  2\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "Epoch: 2 - Step: 0 - MSE loss: 0.09324068 - KL loss: 0.0004650008\n",
      "Epoch: 2 - Step: 1 - MSE loss: 0.0940694 - KL loss: 0.0003600206\n",
      "Epoch: 2 - Step: 2 - MSE loss: 0.09505423 - KL loss: 0.00030459225\n",
      "Epoch: 2 - Step: 3 - MSE loss: 0.09641608 - KL loss: 0.00025694072\n",
      "Epoch:  3\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "Epoch: 3 - Step: 0 - MSE loss: 0.09437666 - KL loss: 0.00022980309\n",
      "Epoch: 3 - Step: 1 - MSE loss: 0.09294041 - KL loss: 0.000219222\n",
      "Epoch: 3 - Step: 2 - MSE loss: 0.094076864 - KL loss: 0.00022582771\n",
      "Epoch: 3 - Step: 3 - MSE loss: 0.09062871 - KL loss: 0.00026341502\n",
      "Epoch:  4\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "Epoch: 4 - Step: 0 - MSE loss: 0.09213046 - KL loss: 0.00035933382\n",
      "Epoch: 4 - Step: 1 - MSE loss: 0.08963084 - KL loss: 0.0005934903\n",
      "Epoch: 4 - Step: 2 - MSE loss: 0.088387765 - KL loss: 0.0012846808\n",
      "Epoch: 4 - Step: 3 - MSE loss: 0.08984798 - KL loss: 0.003458175\n",
      "Epoch:  5\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "Epoch: 5 - Step: 0 - MSE loss: 0.087978005 - KL loss: 0.010714807\n",
      "Epoch: 5 - Step: 1 - MSE loss: 0.08662953 - KL loss: 0.039268255\n",
      "Epoch: 5 - Step: 2 - MSE loss: 0.08504424 - KL loss: 0.16292563\n",
      "Epoch: 5 - Step: 3 - MSE loss: 0.08538362 - KL loss: 0.06110957\n",
      "Epoch:  6\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "Epoch: 6 - Step: 0 - MSE loss: 0.082429804 - KL loss: 0.16619304\n",
      "Epoch: 6 - Step: 1 - MSE loss: 0.07760753 - KL loss: 0.07668726\n",
      "Epoch: 6 - Step: 2 - MSE loss: 0.080754004 - KL loss: 0.14110956\n",
      "Epoch: 6 - Step: 3 - MSE loss: 0.07995082 - KL loss: 0.074072495\n",
      "Epoch:  7\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "Epoch: 7 - Step: 0 - MSE loss: 0.07729211 - KL loss: 0.13638857\n",
      "Epoch: 7 - Step: 1 - MSE loss: 0.0789522 - KL loss: 0.051684905\n",
      "Epoch: 7 - Step: 2 - MSE loss: 0.074258365 - KL loss: 0.10171808\n",
      "Epoch: 7 - Step: 3 - MSE loss: 0.08173553 - KL loss: 0.072236404\n",
      "Epoch:  8\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "Epoch: 8 - Step: 0 - MSE loss: 0.07713146 - KL loss: 0.059585854\n",
      "Epoch: 8 - Step: 1 - MSE loss: 0.07445521 - KL loss: 0.110102266\n",
      "Epoch: 8 - Step: 2 - MSE loss: 0.078356616 - KL loss: 0.08157714\n",
      "Epoch: 8 - Step: 3 - MSE loss: 0.07546985 - KL loss: 0.0554551\n",
      "Epoch:  9\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "Epoch: 9 - Step: 0 - MSE loss: 0.075777486 - KL loss: 0.07793597\n",
      "Epoch: 9 - Step: 1 - MSE loss: 0.07568815 - KL loss: 0.071441315\n",
      "Epoch: 9 - Step: 2 - MSE loss: 0.07482187 - KL loss: 0.047967255\n",
      "Epoch: 9 - Step: 3 - MSE loss: 0.07631082 - KL loss: 0.13220915\n",
      "Epoch:  10\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "Epoch: 10 - Step: 0 - MSE loss: 0.0758638 - KL loss: 0.030916743\n",
      "Epoch: 10 - Step: 1 - MSE loss: 0.071398936 - KL loss: 0.059412993\n",
      "Epoch: 10 - Step: 2 - MSE loss: 0.076099604 - KL loss: 0.10343022\n",
      "Epoch: 10 - Step: 3 - MSE loss: 0.07539207 - KL loss: 0.05644435\n",
      "Epoch:  11\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "Epoch: 11 - Step: 0 - MSE loss: 0.07432375 - KL loss: 0.045380816\n",
      "Epoch: 11 - Step: 1 - MSE loss: 0.07415054 - KL loss: 0.07612774\n",
      "Epoch: 11 - Step: 2 - MSE loss: 0.07283196 - KL loss: 0.06357014\n",
      "Epoch: 11 - Step: 3 - MSE loss: 0.07190617 - KL loss: 0.0380123\n",
      "Epoch:  12\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "Epoch: 12 - Step: 0 - MSE loss: 0.071527965 - KL loss: 0.089081235\n",
      "Epoch: 12 - Step: 1 - MSE loss: 0.07104033 - KL loss: 0.04718698\n",
      "Epoch: 12 - Step: 2 - MSE loss: 0.07163957 - KL loss: 0.05449008\n",
      "Epoch: 12 - Step: 3 - MSE loss: 0.07412068 - KL loss: 0.08329609\n",
      "Epoch:  13\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "Epoch: 13 - Step: 0 - MSE loss: 0.07533085 - KL loss: 0.0390571\n",
      "Epoch: 13 - Step: 1 - MSE loss: 0.07187935 - KL loss: 0.13380928\n",
      "Epoch: 13 - Step: 2 - MSE loss: 0.072583 - KL loss: 0.055563577\n",
      "Epoch: 13 - Step: 3 - MSE loss: 0.070798665 - KL loss: 0.05201615\n",
      "Epoch:  14\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "Epoch: 14 - Step: 0 - MSE loss: 0.06889238 - KL loss: 0.073423505\n",
      "Epoch: 14 - Step: 1 - MSE loss: 0.06620491 - KL loss: 0.059391797\n",
      "Epoch: 14 - Step: 2 - MSE loss: 0.07019357 - KL loss: 0.055723518\n",
      "Epoch: 14 - Step: 3 - MSE loss: 0.070125945 - KL loss: 0.06695826\n",
      "Epoch:  15\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "Epoch: 15 - Step: 0 - MSE loss: 0.06976529 - KL loss: 0.089330256\n",
      "Epoch: 15 - Step: 1 - MSE loss: 0.07363973 - KL loss: 0.086060554\n",
      "Epoch: 15 - Step: 2 - MSE loss: 0.06896918 - KL loss: 0.07051207\n",
      "Epoch: 15 - Step: 3 - MSE loss: 0.066158965 - KL loss: 0.06003001\n",
      "Epoch:  16\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "Epoch: 16 - Step: 0 - MSE loss: 0.0659849 - KL loss: 0.04701043\n",
      "Epoch: 16 - Step: 1 - MSE loss: 0.064928815 - KL loss: 0.059936713\n",
      "Epoch: 16 - Step: 2 - MSE loss: 0.065251924 - KL loss: 0.10072335\n",
      "Epoch: 16 - Step: 3 - MSE loss: 0.06535502 - KL loss: 0.0675208\n",
      "Epoch:  17\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "Epoch: 17 - Step: 0 - MSE loss: 0.066813305 - KL loss: 0.079677604\n",
      "Epoch: 17 - Step: 1 - MSE loss: 0.06809419 - KL loss: 0.10150923\n",
      "Epoch: 17 - Step: 2 - MSE loss: 0.0627369 - KL loss: 0.03940004\n",
      "Epoch: 17 - Step: 3 - MSE loss: 0.062387504 - KL loss: 0.0792707\n",
      "Epoch:  18\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "Epoch: 18 - Step: 0 - MSE loss: 0.06217241 - KL loss: 0.057063803\n",
      "Epoch: 18 - Step: 1 - MSE loss: 0.061198622 - KL loss: 0.09107512\n",
      "Epoch: 18 - Step: 2 - MSE loss: 0.05911225 - KL loss: 0.0924744\n",
      "Epoch: 18 - Step: 3 - MSE loss: 0.061096996 - KL loss: 0.07111159\n",
      "Epoch:  19\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "Epoch: 19 - Step: 0 - MSE loss: 0.058556404 - KL loss: 0.08431928\n",
      "Epoch: 19 - Step: 1 - MSE loss: 0.05710681 - KL loss: 0.08583942\n",
      "Epoch: 19 - Step: 2 - MSE loss: 0.060040254 - KL loss: 0.070371255\n",
      "Epoch: 19 - Step: 3 - MSE loss: 0.060140934 - KL loss: 0.08486995\n",
      "Epoch:  20\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "Epoch: 20 - Step: 0 - MSE loss: 0.057050575 - KL loss: 0.07371748\n",
      "Epoch: 20 - Step: 1 - MSE loss: 0.06268316 - KL loss: 0.07246038\n",
      "Epoch: 20 - Step: 2 - MSE loss: 0.060025454 - KL loss: 0.07658006\n",
      "Epoch: 20 - Step: 3 - MSE loss: 0.059080824 - KL loss: 0.064851455\n",
      "Epoch:  21\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "Epoch: 21 - Step: 0 - MSE loss: 0.05659074 - KL loss: 0.100972\n",
      "Epoch: 21 - Step: 1 - MSE loss: 0.058031768 - KL loss: 0.091379225\n",
      "Epoch: 21 - Step: 2 - MSE loss: 0.057787757 - KL loss: 0.089423\n",
      "Epoch: 21 - Step: 3 - MSE loss: 0.05614054 - KL loss: 0.0988062\n",
      "Epoch:  22\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "Epoch: 22 - Step: 0 - MSE loss: 0.057744596 - KL loss: 0.09570703\n",
      "Epoch: 22 - Step: 1 - MSE loss: 0.06683388 - KL loss: 0.12828611\n",
      "Epoch: 22 - Step: 2 - MSE loss: 0.056855913 - KL loss: 0.0628114\n",
      "Epoch: 22 - Step: 3 - MSE loss: 0.05704831 - KL loss: 0.08640345\n",
      "Epoch:  23\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "Epoch: 23 - Step: 0 - MSE loss: 0.056809384 - KL loss: 0.08266935\n",
      "Epoch: 23 - Step: 1 - MSE loss: 0.05797273 - KL loss: 0.066926405\n",
      "Epoch: 23 - Step: 2 - MSE loss: 0.058038693 - KL loss: 0.08578088\n",
      "Epoch: 23 - Step: 3 - MSE loss: 0.059183653 - KL loss: 0.06317157\n",
      "Epoch:  24\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "Epoch: 24 - Step: 0 - MSE loss: 0.05797884 - KL loss: 0.077001475\n",
      "Epoch: 24 - Step: 1 - MSE loss: 0.056163117 - KL loss: 0.08732514\n",
      "Epoch: 24 - Step: 2 - MSE loss: 0.055733334 - KL loss: 0.07253611\n",
      "Epoch: 24 - Step: 3 - MSE loss: 0.056267694 - KL loss: 0.097788446\n",
      "Epoch:  25\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "Epoch: 25 - Step: 0 - MSE loss: 0.054492157 - KL loss: 0.09717178\n",
      "Epoch: 25 - Step: 1 - MSE loss: 0.055065025 - KL loss: 0.08126958\n",
      "Epoch: 25 - Step: 2 - MSE loss: 0.057169665 - KL loss: 0.08986197\n",
      "Epoch: 25 - Step: 3 - MSE loss: 0.053372268 - KL loss: 0.094567314\n",
      "Epoch:  26\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "Epoch: 26 - Step: 0 - MSE loss: 0.05406542 - KL loss: 0.09272908\n",
      "Epoch: 26 - Step: 1 - MSE loss: 0.05432549 - KL loss: 0.087818444\n",
      "Epoch: 26 - Step: 2 - MSE loss: 0.05332442 - KL loss: 0.10859453\n",
      "Epoch: 26 - Step: 3 - MSE loss: 0.055816803 - KL loss: 0.08862591\n",
      "Epoch:  27\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "Epoch: 27 - Step: 0 - MSE loss: 0.05210133 - KL loss: 0.10156809\n",
      "Epoch: 27 - Step: 1 - MSE loss: 0.058251947 - KL loss: 0.09340112\n",
      "Epoch: 27 - Step: 2 - MSE loss: 0.0564829 - KL loss: 0.09267849\n",
      "Epoch: 27 - Step: 3 - MSE loss: 0.05310475 - KL loss: 0.09359692\n",
      "Epoch:  28\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "Epoch: 28 - Step: 0 - MSE loss: 0.053596973 - KL loss: 0.10038681\n",
      "Epoch: 28 - Step: 1 - MSE loss: 0.05188636 - KL loss: 0.08938597\n",
      "Epoch: 28 - Step: 2 - MSE loss: 0.052355807 - KL loss: 0.11802298\n",
      "Epoch: 28 - Step: 3 - MSE loss: 0.054525517 - KL loss: 0.09719793\n",
      "Epoch:  29\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "Epoch: 29 - Step: 0 - MSE loss: 0.05320707 - KL loss: 0.10025223\n",
      "Epoch: 29 - Step: 1 - MSE loss: 0.05056286 - KL loss: 0.11242068\n",
      "Epoch: 29 - Step: 2 - MSE loss: 0.053073768 - KL loss: 0.11736831\n",
      "Epoch: 29 - Step: 3 - MSE loss: 0.05142104 - KL loss: 0.10974558\n",
      "Epoch:  30\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "Epoch: 30 - Step: 0 - MSE loss: 0.05404338 - KL loss: 0.11947644\n",
      "Epoch: 30 - Step: 1 - MSE loss: 0.05115087 - KL loss: 0.117719576\n",
      "Epoch: 30 - Step: 2 - MSE loss: 0.052811906 - KL loss: 0.08824123\n",
      "Epoch: 30 - Step: 3 - MSE loss: 0.052415546 - KL loss: 0.116597556\n",
      "Epoch:  31\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "Epoch: 31 - Step: 0 - MSE loss: 0.051595222 - KL loss: 0.100623146\n",
      "Epoch: 31 - Step: 1 - MSE loss: 0.051238593 - KL loss: 0.109559715\n",
      "Epoch: 31 - Step: 2 - MSE loss: 0.053313535 - KL loss: 0.12317133\n",
      "Epoch: 31 - Step: 3 - MSE loss: 0.050347436 - KL loss: 0.09550455\n",
      "Epoch:  32\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "Epoch: 32 - Step: 0 - MSE loss: 0.050729413 - KL loss: 0.099283166\n",
      "Epoch: 32 - Step: 1 - MSE loss: 0.051254082 - KL loss: 0.15426567\n",
      "Epoch: 32 - Step: 2 - MSE loss: 0.050276726 - KL loss: 0.105054274\n",
      "Epoch: 32 - Step: 3 - MSE loss: 0.053716823 - KL loss: 0.12522112\n",
      "Epoch:  33\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "Epoch: 33 - Step: 0 - MSE loss: 0.051284183 - KL loss: 0.11606807\n",
      "Epoch: 33 - Step: 1 - MSE loss: 0.05389766 - KL loss: 0.11333184\n",
      "Epoch: 33 - Step: 2 - MSE loss: 0.050008934 - KL loss: 0.11290426\n",
      "Epoch: 33 - Step: 3 - MSE loss: 0.04802977 - KL loss: 0.12775695\n",
      "Epoch:  34\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "Epoch: 34 - Step: 0 - MSE loss: 0.049408525 - KL loss: 0.12196675\n",
      "Epoch: 34 - Step: 1 - MSE loss: 0.04911359 - KL loss: 0.13798729\n",
      "Epoch: 34 - Step: 2 - MSE loss: 0.050055113 - KL loss: 0.13597389\n",
      "Epoch: 34 - Step: 3 - MSE loss: 0.049319383 - KL loss: 0.13295522\n",
      "Epoch:  35\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "Epoch: 35 - Step: 0 - MSE loss: 0.049113896 - KL loss: 0.12668039\n",
      "Epoch: 35 - Step: 1 - MSE loss: 0.050216902 - KL loss: 0.12956986\n",
      "Epoch: 35 - Step: 2 - MSE loss: 0.047221635 - KL loss: 0.12880322\n",
      "Epoch: 35 - Step: 3 - MSE loss: 0.04840669 - KL loss: 0.13649811\n",
      "Epoch:  36\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "Epoch: 36 - Step: 0 - MSE loss: 0.048905075 - KL loss: 0.152238\n",
      "Epoch: 36 - Step: 1 - MSE loss: 0.047533434 - KL loss: 0.1390284\n",
      "Epoch: 36 - Step: 2 - MSE loss: 0.04963096 - KL loss: 0.13751219\n",
      "Epoch: 36 - Step: 3 - MSE loss: 0.050055735 - KL loss: 0.14588977\n",
      "Epoch:  37\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "Epoch: 37 - Step: 0 - MSE loss: 0.050392598 - KL loss: 0.11367835\n",
      "Epoch: 37 - Step: 1 - MSE loss: 0.048734237 - KL loss: 0.11346844\n",
      "Epoch: 37 - Step: 2 - MSE loss: 0.048672367 - KL loss: 0.14284751\n",
      "Epoch: 37 - Step: 3 - MSE loss: 0.04733865 - KL loss: 0.14138642\n",
      "Epoch:  38\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Epoch: 38 - Step: 0 - MSE loss: 0.04728539 - KL loss: 0.13982175\n",
      "Epoch: 38 - Step: 1 - MSE loss: 0.047834698 - KL loss: 0.13161409\n",
      "Epoch: 38 - Step: 2 - MSE loss: 0.04586212 - KL loss: 0.15937081\n",
      "Epoch: 38 - Step: 3 - MSE loss: 0.047719326 - KL loss: 0.16150536\n",
      "Epoch:  39\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "Epoch: 39 - Step: 0 - MSE loss: 0.049769837 - KL loss: 0.13172653\n",
      "Epoch: 39 - Step: 1 - MSE loss: 0.046559457 - KL loss: 0.16071385\n",
      "Epoch: 39 - Step: 2 - MSE loss: 0.05036199 - KL loss: 0.1269431\n",
      "Epoch: 39 - Step: 3 - MSE loss: 0.04667753 - KL loss: 0.14686976\n",
      "Epoch:  40\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "Epoch: 40 - Step: 0 - MSE loss: 0.0457522 - KL loss: 0.15053424\n",
      "Epoch: 40 - Step: 1 - MSE loss: 0.04661632 - KL loss: 0.15460262\n",
      "Epoch: 40 - Step: 2 - MSE loss: 0.047240928 - KL loss: 0.13959762\n",
      "Epoch: 40 - Step: 3 - MSE loss: 0.045487594 - KL loss: 0.13947396\n",
      "Epoch:  41\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "Epoch: 41 - Step: 0 - MSE loss: 0.04681946 - KL loss: 0.16652967\n",
      "Epoch: 41 - Step: 1 - MSE loss: 0.045713972 - KL loss: 0.15910076\n",
      "Epoch: 41 - Step: 2 - MSE loss: 0.0447657 - KL loss: 0.15706256\n",
      "Epoch: 41 - Step: 3 - MSE loss: 0.04590979 - KL loss: 0.146725\n",
      "Epoch:  42\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "Epoch: 42 - Step: 0 - MSE loss: 0.044367526 - KL loss: 0.1586303\n",
      "Epoch: 42 - Step: 1 - MSE loss: 0.046555683 - KL loss: 0.14258799\n",
      "Epoch: 42 - Step: 2 - MSE loss: 0.047392916 - KL loss: 0.1483672\n",
      "Epoch: 42 - Step: 3 - MSE loss: 0.049621023 - KL loss: 0.15828165\n",
      "Epoch:  43\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "Epoch: 43 - Step: 0 - MSE loss: 0.0500156 - KL loss: 0.13244298\n",
      "Epoch: 43 - Step: 1 - MSE loss: 0.04634912 - KL loss: 0.12894492\n",
      "Epoch: 43 - Step: 2 - MSE loss: 0.046942726 - KL loss: 0.1680205\n",
      "Epoch: 43 - Step: 3 - MSE loss: 0.04421627 - KL loss: 0.17360073\n",
      "Epoch:  44\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "Epoch: 44 - Step: 0 - MSE loss: 0.04607317 - KL loss: 0.15337189\n",
      "Epoch: 44 - Step: 1 - MSE loss: 0.045218498 - KL loss: 0.15154186\n",
      "Epoch: 44 - Step: 2 - MSE loss: 0.045430135 - KL loss: 0.15940547\n",
      "Epoch: 44 - Step: 3 - MSE loss: 0.04412235 - KL loss: 0.15431315\n",
      "Epoch:  45\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "Epoch: 45 - Step: 0 - MSE loss: 0.04570136 - KL loss: 0.1775918\n",
      "Epoch: 45 - Step: 1 - MSE loss: 0.044587772 - KL loss: 0.15332225\n",
      "Epoch: 45 - Step: 2 - MSE loss: 0.045991708 - KL loss: 0.1609886\n",
      "Epoch: 45 - Step: 3 - MSE loss: 0.04400602 - KL loss: 0.14801906\n",
      "Epoch:  46\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "Epoch: 46 - Step: 0 - MSE loss: 0.044442505 - KL loss: 0.17108919\n",
      "Epoch: 46 - Step: 1 - MSE loss: 0.044582155 - KL loss: 0.15216793\n",
      "Epoch: 46 - Step: 2 - MSE loss: 0.046207327 - KL loss: 0.16796942\n",
      "Epoch: 46 - Step: 3 - MSE loss: 0.04664978 - KL loss: 0.15702936\n",
      "Epoch:  47\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "Epoch: 47 - Step: 0 - MSE loss: 0.04842518 - KL loss: 0.14566967\n",
      "Epoch: 47 - Step: 1 - MSE loss: 0.045017917 - KL loss: 0.13760823\n",
      "Epoch: 47 - Step: 2 - MSE loss: 0.044842895 - KL loss: 0.16205013\n",
      "Epoch: 47 - Step: 3 - MSE loss: 0.04478736 - KL loss: 0.17306398\n",
      "Epoch:  48\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "Epoch: 48 - Step: 0 - MSE loss: 0.04693948 - KL loss: 0.1875307\n",
      "Epoch: 48 - Step: 1 - MSE loss: 0.0454776 - KL loss: 0.12740946\n",
      "Epoch: 48 - Step: 2 - MSE loss: 0.045985978 - KL loss: 0.13295937\n",
      "Epoch: 48 - Step: 3 - MSE loss: 0.04312299 - KL loss: 0.17274697\n",
      "Epoch:  49\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "Epoch: 49 - Step: 0 - MSE loss: 0.04345606 - KL loss: 0.17646644\n",
      "Epoch: 49 - Step: 1 - MSE loss: 0.04383016 - KL loss: 0.1795746\n",
      "Epoch: 49 - Step: 2 - MSE loss: 0.04346579 - KL loss: 0.15915442\n",
      "Epoch: 49 - Step: 3 - MSE loss: 0.0427045 - KL loss: 0.17315426\n",
      "Epoch:  50\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "Epoch: 50 - Step: 0 - MSE loss: 0.042451803 - KL loss: 0.17244852\n",
      "Epoch: 50 - Step: 1 - MSE loss: 0.044852436 - KL loss: 0.16907331\n",
      "Epoch: 50 - Step: 2 - MSE loss: 0.04404362 - KL loss: 0.16785297\n",
      "Epoch: 50 - Step: 3 - MSE loss: 0.04170089 - KL loss: 0.16124325\n",
      "Epoch:  51\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "Epoch: 51 - Step: 0 - MSE loss: 0.043645475 - KL loss: 0.1972819\n",
      "Epoch: 51 - Step: 1 - MSE loss: 0.0476803 - KL loss: 0.16421142\n",
      "Epoch: 51 - Step: 2 - MSE loss: 0.045928165 - KL loss: 0.14114384\n",
      "Epoch: 51 - Step: 3 - MSE loss: 0.04502973 - KL loss: 0.14573213\n",
      "Epoch:  52\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "Epoch: 52 - Step: 0 - MSE loss: 0.04335563 - KL loss: 0.16589016\n",
      "Epoch: 52 - Step: 1 - MSE loss: 0.044950288 - KL loss: 0.17518383\n",
      "Epoch: 52 - Step: 2 - MSE loss: 0.043782037 - KL loss: 0.1818621\n",
      "Epoch: 52 - Step: 3 - MSE loss: 0.041462168 - KL loss: 0.16179097\n",
      "Epoch:  53\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "Epoch: 53 - Step: 0 - MSE loss: 0.042702306 - KL loss: 0.1632608\n",
      "Epoch: 53 - Step: 1 - MSE loss: 0.043327183 - KL loss: 0.18209691\n",
      "Epoch: 53 - Step: 2 - MSE loss: 0.04248998 - KL loss: 0.18692315\n",
      "Epoch: 53 - Step: 3 - MSE loss: 0.043078028 - KL loss: 0.1568388\n",
      "Epoch:  54\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "Epoch: 54 - Step: 0 - MSE loss: 0.04291898 - KL loss: 0.18698141\n",
      "Epoch: 54 - Step: 1 - MSE loss: 0.042475224 - KL loss: 0.17354062\n",
      "Epoch: 54 - Step: 2 - MSE loss: 0.04339092 - KL loss: 0.16370375\n",
      "Epoch: 54 - Step: 3 - MSE loss: 0.042528335 - KL loss: 0.19589299\n",
      "Epoch:  55\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "Epoch: 55 - Step: 0 - MSE loss: 0.04313166 - KL loss: 0.17143376\n",
      "Epoch: 55 - Step: 1 - MSE loss: 0.04310858 - KL loss: 0.16100624\n",
      "Epoch: 55 - Step: 2 - MSE loss: 0.04198582 - KL loss: 0.19031566\n",
      "Epoch: 55 - Step: 3 - MSE loss: 0.042863157 - KL loss: 0.17654403\n",
      "Epoch:  56\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "Epoch: 56 - Step: 0 - MSE loss: 0.043075 - KL loss: 0.16795836\n",
      "Epoch: 56 - Step: 1 - MSE loss: 0.041359365 - KL loss: 0.1683077\n",
      "Epoch: 56 - Step: 2 - MSE loss: 0.04145831 - KL loss: 0.18648794\n",
      "Epoch: 56 - Step: 3 - MSE loss: 0.04149693 - KL loss: 0.20271581\n",
      "Epoch:  57\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "Epoch: 57 - Step: 0 - MSE loss: 0.04069357 - KL loss: 0.17768586\n",
      "Epoch: 57 - Step: 1 - MSE loss: 0.041112866 - KL loss: 0.1882017\n",
      "Epoch: 57 - Step: 2 - MSE loss: 0.04100265 - KL loss: 0.19478796\n",
      "Epoch: 57 - Step: 3 - MSE loss: 0.04322693 - KL loss: 0.18292019\n",
      "Epoch:  58\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "Epoch: 58 - Step: 0 - MSE loss: 0.04222219 - KL loss: 0.19218731\n",
      "Epoch: 58 - Step: 1 - MSE loss: 0.040113796 - KL loss: 0.17315218\n",
      "Epoch: 58 - Step: 2 - MSE loss: 0.042056482 - KL loss: 0.18170783\n",
      "Epoch: 58 - Step: 3 - MSE loss: 0.043239966 - KL loss: 0.18890315\n",
      "Epoch:  59\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "Epoch: 59 - Step: 0 - MSE loss: 0.04478742 - KL loss: 0.15756842\n",
      "Epoch: 59 - Step: 1 - MSE loss: 0.04497929 - KL loss: 0.17947356\n",
      "Epoch: 59 - Step: 2 - MSE loss: 0.047280844 - KL loss: 0.16556667\n",
      "Epoch: 59 - Step: 3 - MSE loss: 0.047128774 - KL loss: 0.14986223\n",
      "Epoch:  60\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "Epoch: 60 - Step: 0 - MSE loss: 0.04406017 - KL loss: 0.12371281\n",
      "Epoch: 60 - Step: 1 - MSE loss: 0.043320358 - KL loss: 0.16232024\n",
      "Epoch: 60 - Step: 2 - MSE loss: 0.0432715 - KL loss: 0.19870621\n",
      "Epoch: 60 - Step: 3 - MSE loss: 0.040728148 - KL loss: 0.19648534\n",
      "Epoch:  61\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "Epoch: 61 - Step: 0 - MSE loss: 0.041323226 - KL loss: 0.17282076\n",
      "Epoch: 61 - Step: 1 - MSE loss: 0.043254543 - KL loss: 0.18103239\n",
      "Epoch: 61 - Step: 2 - MSE loss: 0.041670054 - KL loss: 0.18763585\n",
      "Epoch: 61 - Step: 3 - MSE loss: 0.040569026 - KL loss: 0.18485944\n",
      "Epoch:  62\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "Epoch: 62 - Step: 0 - MSE loss: 0.041557763 - KL loss: 0.18607573\n",
      "Epoch: 62 - Step: 1 - MSE loss: 0.040011898 - KL loss: 0.18858421\n",
      "Epoch: 62 - Step: 2 - MSE loss: 0.041610356 - KL loss: 0.19645643\n",
      "Epoch: 62 - Step: 3 - MSE loss: 0.04061865 - KL loss: 0.187804\n",
      "Epoch:  63\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "Epoch: 63 - Step: 0 - MSE loss: 0.039572082 - KL loss: 0.17955235\n",
      "Epoch: 63 - Step: 1 - MSE loss: 0.040404942 - KL loss: 0.18150741\n",
      "Epoch: 63 - Step: 2 - MSE loss: 0.04030222 - KL loss: 0.20361918\n",
      "Epoch: 63 - Step: 3 - MSE loss: 0.042642385 - KL loss: 0.19148284\n",
      "Epoch:  64\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "Epoch: 64 - Step: 0 - MSE loss: 0.03929455 - KL loss: 0.18879414\n",
      "Epoch: 64 - Step: 1 - MSE loss: 0.041016623 - KL loss: 0.19784328\n",
      "Epoch: 64 - Step: 2 - MSE loss: 0.040789653 - KL loss: 0.19841102\n",
      "Epoch: 64 - Step: 3 - MSE loss: 0.040356826 - KL loss: 0.20061336\n",
      "Epoch:  65\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "Epoch: 65 - Step: 0 - MSE loss: 0.040538043 - KL loss: 0.18795943\n",
      "Epoch: 65 - Step: 1 - MSE loss: 0.04252995 - KL loss: 0.18644631\n",
      "Epoch: 65 - Step: 2 - MSE loss: 0.040272456 - KL loss: 0.19909307\n",
      "Epoch: 65 - Step: 3 - MSE loss: 0.041045032 - KL loss: 0.18437389\n",
      "Epoch:  66\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "Epoch: 66 - Step: 0 - MSE loss: 0.039249685 - KL loss: 0.20304917\n",
      "Epoch: 66 - Step: 1 - MSE loss: 0.04037608 - KL loss: 0.19867006\n",
      "Epoch: 66 - Step: 2 - MSE loss: 0.04239996 - KL loss: 0.18037403\n",
      "Epoch: 66 - Step: 3 - MSE loss: 0.0407064 - KL loss: 0.17915346\n",
      "Epoch:  67\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "Epoch: 67 - Step: 0 - MSE loss: 0.04095745 - KL loss: 0.18435201\n",
      "Epoch: 67 - Step: 1 - MSE loss: 0.039060514 - KL loss: 0.19082943\n",
      "Epoch: 67 - Step: 2 - MSE loss: 0.04147284 - KL loss: 0.19547376\n",
      "Epoch: 67 - Step: 3 - MSE loss: 0.041215904 - KL loss: 0.18944967\n",
      "Epoch:  68\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "Epoch: 68 - Step: 0 - MSE loss: 0.04117636 - KL loss: 0.18567011\n",
      "Epoch: 68 - Step: 1 - MSE loss: 0.043209445 - KL loss: 0.19117111\n",
      "Epoch: 68 - Step: 2 - MSE loss: 0.04194356 - KL loss: 0.17517707\n",
      "Epoch: 68 - Step: 3 - MSE loss: 0.040682346 - KL loss: 0.17693865\n",
      "Epoch:  69\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "Epoch: 69 - Step: 0 - MSE loss: 0.039415468 - KL loss: 0.19324225\n",
      "Epoch: 69 - Step: 1 - MSE loss: 0.039688896 - KL loss: 0.19687207\n",
      "Epoch: 69 - Step: 2 - MSE loss: 0.040659346 - KL loss: 0.1944029\n",
      "Epoch: 69 - Step: 3 - MSE loss: 0.040273234 - KL loss: 0.1989463\n",
      "Epoch:  70\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "Epoch: 70 - Step: 0 - MSE loss: 0.04044546 - KL loss: 0.19803983\n",
      "Epoch: 70 - Step: 1 - MSE loss: 0.038839113 - KL loss: 0.20582962\n",
      "Epoch: 70 - Step: 2 - MSE loss: 0.039693203 - KL loss: 0.18472952\n",
      "Epoch: 70 - Step: 3 - MSE loss: 0.038742308 - KL loss: 0.20700687\n",
      "Epoch:  71\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "Epoch: 71 - Step: 0 - MSE loss: 0.03828779 - KL loss: 0.20324515\n",
      "Epoch: 71 - Step: 1 - MSE loss: 0.038637284 - KL loss: 0.19417334\n",
      "Epoch: 71 - Step: 2 - MSE loss: 0.04110615 - KL loss: 0.21101461\n",
      "Epoch: 71 - Step: 3 - MSE loss: 0.043700255 - KL loss: 0.19052798\n",
      "Epoch:  72\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "Epoch: 72 - Step: 0 - MSE loss: 0.041885674 - KL loss: 0.18320543\n",
      "Epoch: 72 - Step: 1 - MSE loss: 0.039528754 - KL loss: 0.1746836\n",
      "Epoch: 72 - Step: 2 - MSE loss: 0.03998592 - KL loss: 0.19361322\n",
      "Epoch: 72 - Step: 3 - MSE loss: 0.039280567 - KL loss: 0.21314354\n",
      "Epoch:  73\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "Epoch: 73 - Step: 0 - MSE loss: 0.038527742 - KL loss: 0.21655497\n",
      "Epoch: 73 - Step: 1 - MSE loss: 0.038077697 - KL loss: 0.1900456\n",
      "Epoch: 73 - Step: 2 - MSE loss: 0.040658414 - KL loss: 0.20781688\n",
      "Epoch: 73 - Step: 3 - MSE loss: 0.038422048 - KL loss: 0.20440485\n",
      "Epoch:  74\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "Epoch: 74 - Step: 0 - MSE loss: 0.038412437 - KL loss: 0.20366597\n",
      "Epoch: 74 - Step: 1 - MSE loss: 0.040096488 - KL loss: 0.20410794\n",
      "Epoch: 74 - Step: 2 - MSE loss: 0.038248744 - KL loss: 0.20458184\n",
      "Epoch: 74 - Step: 3 - MSE loss: 0.039895695 - KL loss: 0.21750902\n",
      "Epoch:  75\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "Epoch: 75 - Step: 0 - MSE loss: 0.03928193 - KL loss: 0.19525298\n",
      "Epoch: 75 - Step: 1 - MSE loss: 0.037927084 - KL loss: 0.19754027\n",
      "Epoch: 75 - Step: 2 - MSE loss: 0.03910941 - KL loss: 0.21955237\n",
      "Epoch: 75 - Step: 3 - MSE loss: 0.039042372 - KL loss: 0.21251391\n",
      "Epoch:  76\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "Epoch: 76 - Step: 0 - MSE loss: 0.04090974 - KL loss: 0.18379423\n",
      "Epoch: 76 - Step: 1 - MSE loss: 0.0365992 - KL loss: 0.19952735\n",
      "Epoch: 76 - Step: 2 - MSE loss: 0.037723314 - KL loss: 0.22063574\n",
      "Epoch: 76 - Step: 3 - MSE loss: 0.040344402 - KL loss: 0.19733058\n",
      "Epoch:  77\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "Epoch: 77 - Step: 0 - MSE loss: 0.038669612 - KL loss: 0.21325763\n",
      "Epoch: 77 - Step: 1 - MSE loss: 0.038488682 - KL loss: 0.20226032\n",
      "Epoch: 77 - Step: 2 - MSE loss: 0.038675968 - KL loss: 0.21389319\n",
      "Epoch: 77 - Step: 3 - MSE loss: 0.03729606 - KL loss: 0.20807235\n",
      "Epoch:  78\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "Epoch: 78 - Step: 0 - MSE loss: 0.03915878 - KL loss: 0.21921307\n",
      "Epoch: 78 - Step: 1 - MSE loss: 0.03985427 - KL loss: 0.19436999\n",
      "Epoch: 78 - Step: 2 - MSE loss: 0.041848358 - KL loss: 0.19917506\n",
      "Epoch: 78 - Step: 3 - MSE loss: 0.043616027 - KL loss: 0.19254503\n",
      "Epoch:  79\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "Epoch: 79 - Step: 0 - MSE loss: 0.043269068 - KL loss: 0.17362532\n",
      "Epoch: 79 - Step: 1 - MSE loss: 0.041219395 - KL loss: 0.18411699\n",
      "Epoch: 79 - Step: 2 - MSE loss: 0.039357323 - KL loss: 0.17600545\n",
      "Epoch: 79 - Step: 3 - MSE loss: 0.040257737 - KL loss: 0.19777723\n",
      "Epoch:  80\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "Epoch: 80 - Step: 0 - MSE loss: 0.038004365 - KL loss: 0.19887304\n",
      "Epoch: 80 - Step: 1 - MSE loss: 0.041053995 - KL loss: 0.20319606\n",
      "Epoch: 80 - Step: 2 - MSE loss: 0.037537295 - KL loss: 0.21952216\n",
      "Epoch: 80 - Step: 3 - MSE loss: 0.03702292 - KL loss: 0.2338974\n",
      "Epoch:  81\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "Epoch: 81 - Step: 0 - MSE loss: 0.037514884 - KL loss: 0.21626054\n",
      "Epoch: 81 - Step: 1 - MSE loss: 0.037618667 - KL loss: 0.2270898\n",
      "Epoch: 81 - Step: 2 - MSE loss: 0.03851213 - KL loss: 0.21485679\n",
      "Epoch: 81 - Step: 3 - MSE loss: 0.037727498 - KL loss: 0.21316962\n",
      "Epoch:  82\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "Epoch: 82 - Step: 0 - MSE loss: 0.03716423 - KL loss: 0.22537616\n",
      "Epoch: 82 - Step: 1 - MSE loss: 0.039241407 - KL loss: 0.21676242\n",
      "Epoch: 82 - Step: 2 - MSE loss: 0.039272174 - KL loss: 0.21417043\n",
      "Epoch: 82 - Step: 3 - MSE loss: 0.03935878 - KL loss: 0.19903755\n",
      "Epoch:  83\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "Epoch: 83 - Step: 0 - MSE loss: 0.03648411 - KL loss: 0.20950969\n",
      "Epoch: 83 - Step: 1 - MSE loss: 0.036863778 - KL loss: 0.22836679\n",
      "Epoch: 83 - Step: 2 - MSE loss: 0.036558494 - KL loss: 0.22033423\n",
      "Epoch: 83 - Step: 3 - MSE loss: 0.038178448 - KL loss: 0.22405377\n",
      "Epoch:  84\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "Epoch: 84 - Step: 0 - MSE loss: 0.03683912 - KL loss: 0.23233283\n",
      "Epoch: 84 - Step: 1 - MSE loss: 0.03572246 - KL loss: 0.2159757\n",
      "Epoch: 84 - Step: 2 - MSE loss: 0.038162187 - KL loss: 0.2265519\n",
      "Epoch: 84 - Step: 3 - MSE loss: 0.03693524 - KL loss: 0.22182576\n",
      "Epoch:  85\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "Epoch: 85 - Step: 0 - MSE loss: 0.03686629 - KL loss: 0.21777429\n",
      "Epoch: 85 - Step: 1 - MSE loss: 0.036737155 - KL loss: 0.22015837\n",
      "Epoch: 85 - Step: 2 - MSE loss: 0.036935788 - KL loss: 0.22810763\n",
      "Epoch: 85 - Step: 3 - MSE loss: 0.039825052 - KL loss: 0.20826763\n",
      "Epoch:  86\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "Epoch: 86 - Step: 0 - MSE loss: 0.039684035 - KL loss: 0.22143571\n",
      "Epoch: 86 - Step: 1 - MSE loss: 0.040159944 - KL loss: 0.19057223\n",
      "Epoch: 86 - Step: 2 - MSE loss: 0.039300036 - KL loss: 0.17212719\n",
      "Epoch: 86 - Step: 3 - MSE loss: 0.040272366 - KL loss: 0.20621583\n",
      "Epoch:  87\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "Epoch: 87 - Step: 0 - MSE loss: 0.038408276 - KL loss: 0.2605923\n",
      "Epoch: 87 - Step: 1 - MSE loss: 0.03778382 - KL loss: 0.21620221\n",
      "Epoch: 87 - Step: 2 - MSE loss: 0.038081568 - KL loss: 0.19224754\n",
      "Epoch: 87 - Step: 3 - MSE loss: 0.03831088 - KL loss: 0.2318142\n",
      "Epoch:  88\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "Epoch: 88 - Step: 0 - MSE loss: 0.036243413 - KL loss: 0.25131813\n",
      "Epoch: 88 - Step: 1 - MSE loss: 0.035757486 - KL loss: 0.23452237\n",
      "Epoch: 88 - Step: 2 - MSE loss: 0.035186186 - KL loss: 0.24451491\n",
      "Epoch: 88 - Step: 3 - MSE loss: 0.03589302 - KL loss: 0.23010997\n",
      "Epoch:  89\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "Epoch: 89 - Step: 0 - MSE loss: 0.035345662 - KL loss: 0.22472069\n",
      "Epoch: 89 - Step: 1 - MSE loss: 0.03624013 - KL loss: 0.23743825\n",
      "Epoch: 89 - Step: 2 - MSE loss: 0.03613859 - KL loss: 0.24190417\n",
      "Epoch: 89 - Step: 3 - MSE loss: 0.037594553 - KL loss: 0.22857498\n",
      "Epoch:  90\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "Epoch: 90 - Step: 0 - MSE loss: 0.037294377 - KL loss: 0.2228131\n",
      "Epoch: 90 - Step: 1 - MSE loss: 0.036308493 - KL loss: 0.22241125\n",
      "Epoch: 90 - Step: 2 - MSE loss: 0.036286164 - KL loss: 0.22449532\n",
      "Epoch: 90 - Step: 3 - MSE loss: 0.03471628 - KL loss: 0.22676496\n",
      "Epoch:  91\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "Epoch: 91 - Step: 0 - MSE loss: 0.035399526 - KL loss: 0.24443272\n",
      "Epoch: 91 - Step: 1 - MSE loss: 0.035274867 - KL loss: 0.24533032\n",
      "Epoch: 91 - Step: 2 - MSE loss: 0.03427199 - KL loss: 0.24110284\n",
      "Epoch: 91 - Step: 3 - MSE loss: 0.033763237 - KL loss: 0.23616824\n",
      "Epoch:  92\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "Epoch: 92 - Step: 0 - MSE loss: 0.03455581 - KL loss: 0.24976286\n",
      "Epoch: 92 - Step: 1 - MSE loss: 0.03407515 - KL loss: 0.2398331\n",
      "Epoch: 92 - Step: 2 - MSE loss: 0.037480433 - KL loss: 0.22275023\n",
      "Epoch: 92 - Step: 3 - MSE loss: 0.041680425 - KL loss: 0.25290972\n",
      "Epoch:  93\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "Epoch: 93 - Step: 0 - MSE loss: 0.039956417 - KL loss: 0.20589715\n",
      "Epoch: 93 - Step: 1 - MSE loss: 0.040639654 - KL loss: 0.1944291\n",
      "Epoch: 93 - Step: 2 - MSE loss: 0.04301927 - KL loss: 0.19679853\n",
      "Epoch: 93 - Step: 3 - MSE loss: 0.038677264 - KL loss: 0.1790714\n",
      "Epoch:  94\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "Epoch: 94 - Step: 0 - MSE loss: 0.036327187 - KL loss: 0.20547917\n",
      "Epoch: 94 - Step: 1 - MSE loss: 0.03708651 - KL loss: 0.23979548\n",
      "Epoch: 94 - Step: 2 - MSE loss: 0.034496028 - KL loss: 0.23747739\n",
      "Epoch: 94 - Step: 3 - MSE loss: 0.036928196 - KL loss: 0.23071468\n",
      "Epoch:  95\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "Epoch: 95 - Step: 0 - MSE loss: 0.035226014 - KL loss: 0.24030647\n",
      "Epoch: 95 - Step: 1 - MSE loss: 0.032976236 - KL loss: 0.22222483\n",
      "Epoch: 95 - Step: 2 - MSE loss: 0.03582697 - KL loss: 0.23582321\n",
      "Epoch: 95 - Step: 3 - MSE loss: 0.03456635 - KL loss: 0.2548456\n",
      "Epoch:  96\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "Epoch: 96 - Step: 0 - MSE loss: 0.035314962 - KL loss: 0.25774553\n",
      "Epoch: 96 - Step: 1 - MSE loss: 0.034021974 - KL loss: 0.24442247\n",
      "Epoch: 96 - Step: 2 - MSE loss: 0.0328187 - KL loss: 0.22706518\n",
      "Epoch: 96 - Step: 3 - MSE loss: 0.03480823 - KL loss: 0.23395167\n",
      "Epoch:  97\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "Epoch: 97 - Step: 0 - MSE loss: 0.034506153 - KL loss: 0.24783616\n",
      "Epoch: 97 - Step: 1 - MSE loss: 0.034230027 - KL loss: 0.2653137\n",
      "Epoch: 97 - Step: 2 - MSE loss: 0.03477804 - KL loss: 0.23835388\n",
      "Epoch: 97 - Step: 3 - MSE loss: 0.03444764 - KL loss: 0.22948939\n",
      "Epoch:  98\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "Epoch: 98 - Step: 0 - MSE loss: 0.035905395 - KL loss: 0.24649265\n",
      "Epoch: 98 - Step: 1 - MSE loss: 0.036943328 - KL loss: 0.23922147\n",
      "Epoch: 98 - Step: 2 - MSE loss: 0.036067154 - KL loss: 0.23085718\n",
      "Epoch: 98 - Step: 3 - MSE loss: 0.035381366 - KL loss: 0.22651252\n",
      "Epoch:  99\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "Epoch: 99 - Step: 0 - MSE loss: 0.035465408 - KL loss: 0.23670267\n",
      "Epoch: 99 - Step: 1 - MSE loss: 0.034664985 - KL loss: 0.22919586\n",
      "Epoch: 99 - Step: 2 - MSE loss: 0.033778533 - KL loss: 0.24521446\n",
      "Epoch: 99 - Step: 3 - MSE loss: 0.03297406 - KL loss: 0.24637364\n",
      "Epoch:  100\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "Epoch: 100 - Step: 0 - MSE loss: 0.032929715 - KL loss: 0.25474507\n",
      "Epoch: 100 - Step: 1 - MSE loss: 0.034954943 - KL loss: 0.26679534\n",
      "Epoch: 100 - Step: 2 - MSE loss: 0.033505764 - KL loss: 0.24343643\n",
      "Epoch: 100 - Step: 3 - MSE loss: 0.03359358 - KL loss: 0.23980203\n"
     ]
    }
   ],
   "source": [
    "random_vector = tf.random.normal(shape=(25, latent_dim))\n",
    "save_images(decoder, 0, 0, random_vector)\n",
    "\n",
    "mse_losses = []\n",
    "kl_losses = []\n",
    "\n",
    "optimizer = Adam(0.0001, 0.5)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    for step, training_batch in enumerate(training_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = model(training_batch)\n",
    "            y_true = tf.reshape(training_batch, shape = [-1])\n",
    "            y_pred = tf.reshape(reconstructed, shape = [-1])\n",
    "            \n",
    "            mse_loss = reconstruction_loss(y_true, y_pred)\n",
    "            mse_losses.append(mse_loss.numpy())\n",
    "            \n",
    "            kl = sum(model.losses)\n",
    "            kl_losses.append(kl.numpy())\n",
    "            \n",
    "            train_loss = 0.01 * kl + mse_loss\n",
    "            \n",
    "            grads = tape.gradient(train_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            if step % 10 == 0:\n",
    "                save_images(decoder, epoch, step, random_vector)\n",
    "            print('Epoch: %s - Step: %s - MSE loss: %s - KL loss: %s' % (epoch, step, mse_loss.numpy(), kl.numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
