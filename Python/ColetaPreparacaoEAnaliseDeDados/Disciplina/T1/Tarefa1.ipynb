{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nome: Matheus Maia da Silva\n",
    "#### Matrícula: 19204136"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Faça um crawler que navegue pelas páginas de países e baixe os htmls."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- O algoritmo desta parte entra na primeira página de países, pega os links da páginas de todos os países e os coloca em um dicionário em que as chaves são os nomes do países. Depois disso, procurar pelo link da próxima página de países e repete o processo até a última página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Faz a requisição do html da primeira página\n",
    "response = requests.get('http://127.0.0.1:8000/places/default/index/')\n",
    "soup = BeautifulSoup(response.content)\n",
    "response.close()\n",
    "\n",
    "# Irá guardar todos os links de referêcia para o botão 'next' (usado para parar o loop)\n",
    "all_page_links = []\n",
    "\n",
    "# Dicionário que irá guardar o link de todos os países\n",
    "pages = {}\n",
    "\n",
    "\n",
    "# Loop que irá navegar pelas páginas de países e irá extrair seus respectivos links\n",
    "while True:\n",
    "    # Cria uma lista com os nomes dos paises\n",
    "    names = [name.text.strip() for name in soup.body.table.find_all('a')]\n",
    "\n",
    "    # Cria uma lista com os links para a página dos países\n",
    "    links = list(map(lambda x: x.get('href'), soup.body.section.find_all('a')))[0:-1]\n",
    "\n",
    "    # Cria um dicionário com o nome dos países, um número de referência e seus links\n",
    "    for idx, name in enumerate(names):\n",
    "        pages.update({name: links[idx]})\n",
    "\n",
    "\n",
    "    # Padrão a ser procurado\n",
    "    pattern = '/places/default/index/'\n",
    "\n",
    "    # Fução que procura um padrão de link\n",
    "    def search_pattern(link):\n",
    "        if pattern in link:\n",
    "            return link\n",
    "        \n",
    "\n",
    "    # Sequência que filtra os links para encontrar apenas o links que contém o botão 'next' e 'previous'\n",
    "    link_next = list(map(lambda x: x['href'], soup.body.section.find_all('a')))\n",
    "    link_next = list(map(search_pattern, link_next))\n",
    "    link_next = list(filter(None, link_next))\n",
    "\n",
    "\n",
    "    # Para o loop quando o único link for o 'previous' (ou seja, quando um link repetido aparecer)\n",
    "    if link_next[-1] in all_page_links:\n",
    "        break\n",
    "    else:\n",
    "        all_page_links.append(link_next[-1])\n",
    "\n",
    "\n",
    "    # Faz o requerimento e extrai o html da página próxima página\n",
    "    soup = BeautifulSoup(requests.get('http://127.0.0.1:8000' + link_next[-1]).content)\n",
    "\n",
    "\n",
    "print(f'Tamanho do dicionário de países: {len(pages)}')\n",
    "display(pages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Faça scraping dos htmls baixados e salve os dados retirados em um arquivo csv. Salvar uma coluna extra no csv contendo um timestamp do momento no qual os dados foram obtidos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- O bloco abaixo cria um dataframe o pandas onde os dados de todos os países serão armazenados para posteriormente criar o CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz uma requisição do primeiro país para extrair as colunas do dataframe\n",
    "response = requests.get('http://127.0.0.1:8000/places/default/view/Afghanistan-1')\n",
    "soup = BeautifulSoup(response.content)\n",
    "response.close()\n",
    "\n",
    "# Extrai as informações do país que está no html\n",
    "infos = [info.text.strip() for info in soup.body.table]\n",
    "\n",
    "\n",
    "# Criação do dataframe usando pandas\n",
    "columns = []\n",
    "for i in infos:\n",
    "    columns.append(i.split(':')[0].lower().replace(' ', '_'))\n",
    "columns.append('timestamp')\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "print('Dataframe vazio: ')\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- Abaixo são feita as requisições das páginas de todos os países, extraindo suas informações e preenchendo o dataframe. Após isso é criado o arquivo CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country, link in pages.items():\n",
    "    # Faz uma requisição da página do país\n",
    "    response = requests.get('http://127.0.0.1:8000' + link)\n",
    "    response.close()\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    \n",
    "\n",
    "\n",
    "    # Extrai os dados\n",
    "    data = [tag.text.split(':')[-1].strip() for tag in soup.body.section.div.form.table.find_all('tr')]\n",
    "    data.append(response.headers['Date'])\n",
    "\n",
    "\n",
    "    # Faz a limpeza dos dados\n",
    "    data[0] = None\n",
    "    data[1] = int(data[1].split(' ')[0])\n",
    "    data[2] = int(data[2])\n",
    "    data[11] = data[12] = None\n",
    "    data[13] = data[13].replace(',', ' ')\n",
    "\n",
    "    def check_if_empty(value):\n",
    "        if value == '':\n",
    "            return None\n",
    "        else:\n",
    "            return value\n",
    "        \n",
    "    data = list(map(check_if_empty, data))\n",
    "\n",
    "\n",
    "    # Acrescenta uma nova linha ao dataframe\n",
    "    new_row = pd.DataFrame(dict(zip(columns, data)), index=[0])\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "\n",
    "# Converte o dataframe em CSV\n",
    "df.to_csv('countries.csv', index=False)\n",
    "\n",
    "\n",
    "response.close()\n",
    "display(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
